{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin, we will change a few settings to make the notebook look a bit prettier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style> body {font-family: \"Calibri\", cursive, sans-serif;} </style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style> body {font-family: \"Calibri\", cursive, sans-serif;} </style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../multimedia/grimace_gauntlet.PNG\" width=\"150\" align=\"right\">\n",
    "\n",
    "# Thanos or Grimace?<br>Classifying Purple Fiction Characters\n",
    "---\n",
    "In this notebook, I will create a CNN framework that classifies comic characters\n",
    "images as \"hero\" or \"villain\". For more info, take a look at the [README file](../README.md). \n",
    "\n",
    "Alright, let's get started.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminaries\n",
    "First, let's import all the relevant packages, configure some plotting options, and define some basic (path) variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import pathlib\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras import Model, layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Import user-defined scripts.\n",
    "PATH_SCRIPTS = os.path.join('..', 'scripts')\n",
    "if PATH_SCRIPTS not in sys.path:\n",
    "    sys.path.append(PATH_SCRIPTS)\n",
    "import purplefunctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Plotting options.\n",
    "mpl.rcParams['font.sans-serif'] = 'Calibri'\n",
    "mpl.rcParams['font.family'] = 'sans-serif'\n",
    "sns.set(font_scale=1.75)\n",
    "sns.set(font = 'Calibri')\n",
    "sns.set_style('ticks')\n",
    "plt.rc('axes.spines', top=False, right=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "PATH_DATA = pathlib.Path(r'../data')\n",
    "PATH_MODELS = pathlib.Path(r'../models')\n",
    "if not PATH_MODELS.exists():\n",
    "    PATH_MODELS.mkdir()\n",
    "    print(\"Created directory \" + str(PATH_MODELS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "# Data\n",
    "## Data Fetching\n",
    "Data was downloaded using [Fatkun Batch Download Image](https://chrome.google.com/webstore/detail/fatkun-batch-download-ima/nnjjahlikiabnchcpehcpkdeckfgnohf)\n",
    "a Google Chrome extension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Data Structuring\n",
    "Lastly, we will structure the images of interest as required by Keras.\n",
    "We need to \"manually\" (i.e., write the code for it) split our data into \n",
    "training and validation directories.\n",
    "\n",
    "```\n",
    "|-- training\n",
    "   |-- thanos\n",
    "   |-- grimace\n",
    "|-- validation\n",
    "   |-- thanos\n",
    "   |-- grimace\n",
    "```\n",
    "\n",
    "Recently, a [new functionality was implemented](https://kylewbanks.com/blog/train-validation-split-with-imagedatagenerator-keras)\n",
    "that allows you to randomly split your data just by specifying what \n",
    "percentage should be used for the validation. Although this saves us\n",
    "from structuring the data, it doesn't allow for data augmentation. Since we \n",
    "don't have that much data, this isn't an option. Therefore, we will be\n",
    "sticking with the classic approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "purplefunctions.structure_images(PATH_DATA, val_prop=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification using CNNs\n",
    "Now, we will use TensorFlow 2.0 to generate a CNN for our task.\n",
    "I like using Keras as a front-end for TensorFlow. However, with this new\n",
    "release it is recommended to drop Keras and move towards [`tf.keras`](https://www.tensorflow.org/guide/keras).\n",
    "Therefore, we will be doing that here. It is worth mentioning\n",
    "that for our purposes, [they are quite similar](https://www.pyimagesearch.com/2019/10/21/keras-vs-tf-keras-whats-the-difference-in-tensorflow-2-0/).\n",
    "\n",
    "## Create data generators\n",
    "We could manually load the images, pre-process them, and make them ready\n",
    "for our task. However, `tf.keras` has an [ImageDataGenerator class](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator)\n",
    "that will save us lots of trouble. \n",
    "\n",
    "First, we will create an ImageDataGenerator instance where we will define\n",
    "the augmentation operations that we want. It is important to mention that\n",
    "with `tf.keras` we have no control over the order in which the data\n",
    "augmentation operations will be executed. We will also define the function\n",
    "that will be implied on each input (`preprocessing_function`). Notice that\n",
    "this function is model specific. Since we will be using the ResNet-50 model,\n",
    "we will use its corresponding function (as defined in the preliminaries).\n",
    "\n",
    "Afterwards, we will apply the method [`flow_from_directory`](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator#flow_from_directory), \n",
    "which will will generate batches of augmented data based on the data located \n",
    "in the given path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_datagen = ImageDataGenerator(\n",
    "        rotation_range=15, # Rotate images randomly withing this range (+/-).\n",
    "        width_shift_range=0.1, # Translate images horizontally randomly within this proportion.\n",
    "        height_shift_range=0.1, # Translate images vertically randomly within this proportion.\n",
    "        shear_range=5, # Shear intensity (shear angle, [deg]).\n",
    "        zoom_range=0.1, # Range for random zoom.\n",
    "        vertical_flip=True,\n",
    "        horizontal_flip=True,\n",
    "        preprocessing_function=preprocess_input\n",
    "        )\n",
    "\n",
    "training_generator = training_datagen.flow_from_directory(\n",
    "        directory=PATH_DATA/'training',\n",
    "        batch_size=64, # Number of images per batch. Arbitrary.\n",
    "        shuffle=True,\n",
    "        class_mode='binary', # We have two possible outputs (hero or villain).\n",
    "        target_size=(224, 224) # ResNet-50 requires these dimensions.\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will create the generator for the validation set. It is pretty\n",
    "much the same that for the training set, except we won't perform any\n",
    "data augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "validation_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        directory=PATH_DATA/'validation',\n",
    "        batch_size=32,\n",
    "        class_mode='binary',\n",
    "        shuffle=False,\n",
    "        target_size=(224, 224)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Create the (pre-trained) network\n",
    "After having our data generators in place, we will actually create our network.\n",
    "As mentioned earlier, we will use the [ResNet-50 model](https://www.tensorflow.org/api_docs/python/tf/keras/applications/ResNet50)\n",
    "as a base. Namely, we will use its already trained convolutional layers and \n",
    "adapt (i.e., train) the last two dense layers. This concept of using\n",
    "(and adapting) a pre-trained model is known as _transfer learning_.\n",
    "\n",
    "\n",
    "### Load pre-trained network\n",
    "Notice this might take a few (~5) minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_pretrained = ResNet50(include_top=False, # Whether to include the fully-connected layer at the top (or not)\n",
    "                          weights='imagenet') # Weights were obtained trained on ImageNet.\n",
    "\n",
    "#cnn_pretrained = vgg19.VGG19(include_top=False, # Whether to include the fully-connected layer at the top (or not)\n",
    "#                          weights='imagenet') # Weights were obtained trained on ImageNet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see a summary of the network architecture by typing `cnn_pretrained.summary()`.\n",
    "Among other things, you will see something like this:\n",
    "\n",
    "```\n",
    "Total params: 23,587,712\n",
    "Trainable params: 23,534,592\n",
    "Non-trainable params: 53,120\n",
    "```\n",
    "\n",
    "Wow, that's a lot of parameters!\n",
    "\n",
    "### Freeze convolutional parameters.\n",
    "Afterwards, we will [freeze the convolutional layers](https://github.com/keras-team/keras/issues/4465#issuecomment-311000870)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for conv_layer in cnn_pretrained.layers:\n",
    "    conv_layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run `cnn_pretrained.summary()`, you will notice how there are no more\n",
    "trainable parameters, since they are frozen now.\n",
    "    \n",
    "```\n",
    "Total params: 23,587,712\n",
    "Trainable params: 0\n",
    "Non-trainable params: 23,587,712\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate classification head\n",
    "We can think of the convolutional layers as a feature extractor. Now, \n",
    "we need to add a classifier on top of it and train it. This part is know\n",
    "as the _classification head_. For the latter, we will use an average layer\n",
    "and two dense layers.\n",
    "\n",
    "First, we will fetch the output layer of the pretrained CNN.\n",
    "Then, we will use `tf.keras`'s [`GlobalAveragePooling2D`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GlobalAveragePooling2D) \n",
    "to average over the spatial locations and convert the features to a\n",
    "1D vector (per image). Afterwards, we will add two dense layers.\n",
    "Notice how `tf.keras` allows us to chain the layers very easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "x = cnn_pretrained.output\n",
    "x = layers.GlobalAveragePooling2D()(x) # Global average layer\n",
    "#x = layers.Dense(256, activation='relu')(x) # Dense layer\n",
    "x = layers.Dense(64, activation='relu')(x) # Dense layer\n",
    "x = layers.Dense(32, activation='relu')(x) # Dense layer\n",
    "x = layers.Dense(1, activation='sigmoid')(x) # Prediction (output) layer\n",
    "model = Model(cnn_pretrained.input, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will define our model's optimizer, loss function, and metric\n",
    "and compile the whole thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam() # Adam = RMSprop + Momentum (https://www.dlology.com/blog/quick-notes-on-how-to-choose-optimizer-in-keras/)\n",
    "loss = 'binary_crossentropy' # Since it is a binary classification (hero or villain)\n",
    "metrics = ['accuracy']\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`model.summary()` shows us that we have additional trainable parameters:\n",
    "\n",
    "```\n",
    "Total params: 23,718,978\n",
    "Trainable params: 131,266\n",
    "Non-trainable params: 23,587,712\n",
    "```\n",
    "These correspond to the output layers. It is always a good sanity check\n",
    "to verify where do these parameters come from. In our case:\n",
    "\n",
    "* $2048 \\times 64$ weights from the global to the dense layer\n",
    "\n",
    "* $64 \\times 2$ weights from the dense to the prediction layer\n",
    "\n",
    "* $64$ and $2$ biases from the dense and the prediction layer, respectively\n",
    "\n",
    "Which adds to 131,266 parameters. Looks like we are good!\n",
    "\n",
    "## Model training\n",
    "Fortunately, Keras makes training a model very easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "n_epochs = 5\n",
    "training_history = model.fit_generator(\n",
    "        generator=training_generator,\n",
    "        epochs=n_epochs,\n",
    "        validation_data=validation_generator)\n",
    "\n",
    "# Save the model.\n",
    "model.save(PATH_MODELS/('resnet50_64_32_1_epochs=' + str(n_epochs) + '_vprop=0.25.h5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that training the model is very likely what will take the longest,\n",
    "specially for a large number of epochs.\n",
    "\n",
    "### Learning curves\n",
    "Now, we will take a look at how the model training evolved. We will do so\n",
    "by taking a look at the learning curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(1, n_epochs+1)\n",
    "fig, ax = plt.subplots(2, 1, figsize=[8, 8])\n",
    "\n",
    "# Accuracy\n",
    "ax[0].plot(epochs, training_history.history['accuracy'], \n",
    "  linewidth=3, label=\"Training accuracy\")\n",
    "ax[0].plot(epochs, training_history.history['val_accuracy'], \n",
    "  linewidth=3, label=\"Validation accuracy\")\n",
    "ax[0].legend(loc=(1.04, 0.75), frameon=False)\n",
    "#ax[0].set_ylim([0, 1])\n",
    "ax[0].set_ylabel(\"Accuracy\", fontweight='bold')\n",
    "\n",
    "# Loss\n",
    "ax[1].plot(epochs, training_history.history['loss'], \n",
    "  linewidth=3, label=\"Training loss\")\n",
    "ax[1].plot(epochs, training_history.history['val_loss'], \n",
    "  linewidth=3, label=\"Validation loss\")\n",
    "ax[1].legend(loc=(1.04, 0.75), frameon=False)\n",
    "#ax[1].set_ylim([0, 1])\n",
    "ax[1].set_xlabel(\"Epoch\", fontweight='bold')\n",
    "ax[1].set_ylabel(\"Loss\", fontweight='bold')\n",
    "\n",
    "fig.savefig(PATH_MODELS/('resnet50_64_32_1_epochs=' + str(n_epochs) + '_vprop=0.25.pdf'), bbox_inches='tight', dpi=150)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
